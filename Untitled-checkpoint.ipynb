{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91887c94-fb75-4f07-887d-7f326df66919",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m avg, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import avg, min, max\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b9797-2a95-4d3c-bdcd-70cad246c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# خد schema من ملف CSV مرة واحدة (batch)\n",
    "sample_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"data/greenhouse_data_20251003_224720.csv\")\n",
    "schema = sample_df.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ee152-ba8a-49e2-a10d-400bcf7cd20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "         .option(\"header\", True)\n",
    "         .schema(schema)   # لازم schema\n",
    "         .csv(\"data/\")     # Spark هيراقب الفولدر ده\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e9838-a122-48a8-adaf-2fb37b48569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union data (assuming نفس الأعمدة موجودة في الاتنين)\n",
    "df = csv_df.unionByName(json_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fffea-b7a8-4d9a-b14f-a2edb94e56dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timestamp', 'string'),\n",
       " ('date', 'string'),\n",
       " ('time', 'string'),\n",
       " ('season', 'string'),\n",
       " ('day_period', 'string'),\n",
       " ('daytime', 'string'),\n",
       " ('soil_temperature_c', 'double'),\n",
       " ('air_temperature_c', 'double'),\n",
       " ('soil_humidity_percent', 'double'),\n",
       " ('air_humidity_percent', 'double'),\n",
       " ('soil_ph', 'double'),\n",
       " ('soil_salinity_ds_m', 'double'),\n",
       " ('light_intensity_lux', 'double'),\n",
       " ('water_level_percent', 'double'),\n",
       " ('location', 'string'),\n",
       " ('is_error', 'boolean')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numeric columns data from string into numeric \n",
    "numeric_cols = [\n",
    "    \"soil_temperature_c\",\"air_temperature_c\",\n",
    "    \"soil_humidity_percent\",\"air_humidity_percent\",\n",
    "    \"soil_ph\",\"soil_salinity_ds_m\",\n",
    "    \"light_intensity_lux\",\"water_level_percent\"\n",
    "]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df = df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806cdc9-cf01-4b93-a82d-7591cd1f31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Dublicates if there \n",
    "\n",
    "df = df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ed52d-cf8e-4bdf-9603-e52227c648ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN Flag anomalies (refer to outliers) (anomaly , normal)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"anomaly_flag\",\n",
    "    when((col(\"soil_temperature_c\") < 0) | (col(\"soil_temperature_c\") > 60), \"temp_anomaly\")\n",
    "    .when((col(\"air_temperature_c\") < -10) | (col(\"air_temperature_c\") > 50), \"air_anomaly\")\n",
    "    .when((col(\"soil_ph\") < 3) | (col(\"soil_ph\") > 9), \"ph_anomaly\")\n",
    "    .when((col(\"soil_humidity_percent\") < 10) | (col(\"soil_humidity_percent\") > 90), \"humidity_anomaly\")\n",
    "    .otherwise(\"normal\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb79f77-5499-4e6b-beb8-0509f3ca5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# داتا فريم جديد للإحصائيات اليومية\n",
    "daily_stats = df.groupBy(\"date\").agg(\n",
    "    avg(\"soil_temperature_c\").alias(\"avg_soil_temp\"),\n",
    "    min(\"soil_temperature_c\").alias(\"min_soil_temp\"),\n",
    "    max(\"soil_temperature_c\").alias(\"max_soil_temp\"),\n",
    "\n",
    "    avg(\"air_temperature_c\").alias(\"avg_air_temp\"),\n",
    "    min(\"air_temperature_c\").alias(\"min_air_temp\"),\n",
    "    max(\"air_temperature_c\").alias(\"max_air_temp\"),\n",
    "\n",
    "    avg(\"soil_humidity_percent\").alias(\"avg_soil_humidity\"),\n",
    "    min(\"soil_humidity_percent\").alias(\"min_soil_humidity\"),\n",
    "    max(\"soil_humidity_percent\").alias(\"max_soil_humidity\"),\n",
    "\n",
    "    avg(\"air_humidity_percent\").alias(\"avg_air_humidity\"),\n",
    "    min(\"air_humidity_percent\").alias(\"min_air_humidity\"),\n",
    "    max(\"air_humidity_percent\").alias(\"max_air_humidity\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ed6d1-58f7-4da4-96db-c053024aebbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
